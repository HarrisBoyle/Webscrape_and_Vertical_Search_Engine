{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d05f17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68f2eec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><p>User-Agent: *\n",
       "Crawl-Delay: 1\n",
       "Disallow: /*?*format=rss\n",
       "Disallow: /*?*export=xls\n",
       "Sitemap: https://pureportal.coventry.ac.uk/sitemap.xml</p></body></html>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get robots.txt file\n",
    "robots = requests.get('https://pureportal.coventry.ac.uk/robots.txt')\n",
    "robots = BeautifulSoup(robots.text)\n",
    "\n",
    "robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4108470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the end page of the publications on which to search\n",
    "html = requests.get(\"https://pureportal.coventry.ac.uk/en/organisations/school-of-economics-finance-and-accounting/publications/?page=0\")\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "for item in soup.find_all('a', {'class':'step'}):\n",
    "    #iterates through an ends at the last step found in the class (thus the last page)\n",
    "    x = item.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa0f3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page number: 0\n",
      "Processing page number: 1\n",
      "Processing page number: 2\n",
      "Processing page number: 3\n",
      "Processing page number: 4\n",
      "Processing page number: 5\n",
      "Processing page number: 6\n",
      "Processing page number: 7\n",
      "Processing page number: 8\n",
      "Processing page number: 9\n",
      "Processing page number: 10\n",
      "Processing page number: 11\n",
      "Processing page number: 12\n",
      "Processing page number: 13\n"
     ]
    }
   ],
   "source": [
    "#set page to 0 and create an empty list of links\n",
    "page = 0\n",
    "link = []\n",
    "\n",
    "#iterate through the pages on publications between page 0 and the max page found above\n",
    "while page >=0 and page <= int(x):\n",
    "    url = f\"https://pureportal.coventry.ac.uk/en/organisations/school-of-economics-finance-and-accounting/publications/?page={page}\"\n",
    "    #Wait 1 second (per robots.txt)\n",
    "    time.sleep(1.0)\n",
    "    print('Processing page number:',page)\n",
    "    r = requests.get(url)\n",
    "    html = r.content\n",
    "    #create a soup that contains all the html from the current page\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    #iterate through the soup to find all 'a' references\n",
    "    for i in soup.find('ul', {'class':'list-results'}).find_all('a'):\n",
    "        #scrape all web links from this and append to the list \"link\"\n",
    "        i['href'] = i['href']\n",
    "        link.append(i['href'])\n",
    "    #Repeat for the next page\n",
    "    \n",
    "    page = page + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90bf2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of just the publication links\n",
    "publications = [x for x in link if \"publications\" in x]\n",
    "\n",
    "#Create a list of just the people profile links\n",
    "persons = [x for x in link if \"persons\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5697d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 Added Publication: https://pureportal.coventry.ac.uk/en/publications/evaluation-of-clinical-interventions-effectiveness-efficiency-and-2\n",
      "633 Added blank Overview field\n",
      "634 Added Publication: https://pureportal.coventry.ac.uk/en/publications/market-orientation-in-the-uk-higher-education-sector-the-influenc-2\n",
      "634 Added blank Overview field\n",
      "635 Added Publication: https://pureportal.coventry.ac.uk/en/publications/developing-a-comprehensive-cross-country-economic-growth-database-2\n",
      "636 Added Publication: https://pureportal.coventry.ac.uk/en/publications/measuring-value-added-in-higher-education-a-proposal-2\n",
      "636 Added blank Overview field\n",
      "637 Added Publication: https://pureportal.coventry.ac.uk/en/publications/modelling-optimal-plant-size-and-market-equilibria-under-differen-2\n",
      "637 Added blank Overview field\n"
     ]
    }
   ],
   "source": [
    "#Create an empty list of the documents to search if it does not exist already\n",
    "while True:\n",
    "    try:\n",
    "        documents\n",
    "        break\n",
    "    except NameError:\n",
    "        documents = []\n",
    "        \n",
    "#Create an empty list of the titles if it does not exist already\n",
    "while True:\n",
    "    try:\n",
    "        titles\n",
    "        break\n",
    "    except NameError:\n",
    "        titles = []\n",
    "        \n",
    "#Create an empty list of all authors of a publication if it does not exist already\n",
    "while True:\n",
    "    try:\n",
    "        authors\n",
    "        break\n",
    "    except NameError:\n",
    "        authors = []\n",
    "        \n",
    "#Create an empty list of the author profiles if it does not exist already\n",
    "while True:\n",
    "    try:\n",
    "        author_profile\n",
    "        break\n",
    "    except NameError:\n",
    "        author_profile = []\n",
    "        \n",
    "#Create an empty list of the publication dates if it does not exist already\n",
    "while True:\n",
    "    try:\n",
    "        dates\n",
    "        break\n",
    "    except NameError:\n",
    "        dates = []\n",
    "        \n",
    "#Create the Library DataFrame if it does not exist\n",
    "while True:\n",
    "    try:\n",
    "        Library\n",
    "        break\n",
    "    except NameError:\n",
    "        Library = pd.DataFrame(list(zip(titles,authors,dates,publications,author_profile,documents)), \n",
    "                               columns=['Title',\n",
    "                                        'Authors',\n",
    "                                        'Publication Date',\n",
    "                                        'Web Link',\n",
    "                                        'Author Profile',\n",
    "                                        'Overview'])\n",
    "\n",
    "\n",
    "#iterate over the publications list \n",
    "for n,i in enumerate(publications):\n",
    "    \n",
    "    #Create a skip if the publication is already contained in the Library\n",
    "    if i not in list(Library['Web Link']):\n",
    "        print(n, 'Added Publication:',i)\n",
    "        r = requests.get(i)\n",
    "        #Wait 1 second (per robots.txt)\n",
    "        time.sleep(1.0)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        #create a blank list of the overview / blurbs from the publications\n",
    "        blurb = []\n",
    "        #find the relevant class (if it does not exist then skip)\n",
    "        if soup.find('div', {'class':'textblock'}) is not None:\n",
    "            #append each blurb to the blurb list\n",
    "            for i in soup.find_all('div', {'class':'textblock'}):\n",
    "                #print(n)\n",
    "                blurb.append(i.text)\n",
    "            #append all the blurbs to the documents list\n",
    "            documents.append(' '.join(blurb))\n",
    "        else: \n",
    "            print(n,'Added blank Overview field')\n",
    "            #add N/A if blank\n",
    "            documents.append(' '.join('N/A'))\n",
    "\n",
    "\n",
    "        #create a blank list of the titles from the publications\n",
    "        t = []\n",
    "        #find the relevant class (if it does not exist then skip)\n",
    "        if soup.find('div', {'class':'rendering'}) is not None:\n",
    "            #append each title to the t list\n",
    "            for i in soup.find('div', {'class':'rendering'}):\n",
    "                #print(n)\n",
    "                t.append(i.text)\n",
    "            #append all the titles to the titles list\n",
    "            titles.append(' '.join(t))\n",
    "        else: \n",
    "            print(n,'Added blank Title field')\n",
    "            #add N/A if blank\n",
    "            titles.append(' '.join('N/A'))\n",
    "\n",
    "\n",
    "        #create a blank list of the authors from the publications\n",
    "        autha = []\n",
    "        #find the relevant class (if it does not exist then skip)\n",
    "        if soup.find_all('p',{'class':'relations persons'}) is not None:\n",
    "            #append each author to the autha list\n",
    "            for i in soup.find_all('p',{'class':'relations persons'}):\n",
    "                #print(n)\n",
    "                autha.append(i.text)\n",
    "            #append all the authors to the authors list\n",
    "            authors.append(' '.join(autha))\n",
    "        else: \n",
    "            print(n,'Added blank Author field')\n",
    "            #add N/A if blank\n",
    "            authors.append(' '.join('N/A'))\n",
    "\n",
    "\n",
    "        #create a blank list of the author profiles from the publications\n",
    "        auth = []\n",
    "        #find the relevant class (if it does not exist then skip)\n",
    "        if soup.find_all('a',{'class':'link person'}) is not None:\n",
    "            #append each author profile to the auth list\n",
    "            for i in soup.find_all('a',{'class':'link person'}):\n",
    "                #print(n)\n",
    "                i['href'] = i['href']\n",
    "                auth.append(i['href'])\n",
    "            #append all the author profiles to the author_profile list\n",
    "            author_profile.append(' '.join(auth))\n",
    "        else: \n",
    "            print(n,'Added blank Author Profile field')\n",
    "            #add N/A if blank\n",
    "            author_profile.append(' '.join('N/A'))\n",
    "\n",
    "\n",
    "        #create a blank list of the dates of the publications\n",
    "        d = []\n",
    "        #find the relevant class (if it does not exist then skip)\n",
    "        if soup.find('span',{'class':'date'}) is not None:\n",
    "            #append each date to the d list\n",
    "            for i in soup.find('span',{'class':'date'}):\n",
    "                #print(n)\n",
    "                d.append(i.text)\n",
    "            #append all the publication dates to the dates list\n",
    "            dates.append(' '.join(d))\n",
    "        else: \n",
    "            print(n,'Added blank Publication Date field')\n",
    "            #add N/A if blank\n",
    "            dates.append(' '.join('N/A'))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e8a1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "638\n",
      "638\n",
      "638\n",
      "638\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "for i in (titles,authors,dates,publications,author_profile,documents):\n",
    "    print (len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31c9af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate the Library (Data Frame)with all the relevant information\n",
    "Library = pd.DataFrame(list(zip(titles,authors,dates,publications,author_profile,documents)), \n",
    "                            columns=['Title',\n",
    "                                     'Authors',\n",
    "                                     'Publication Date',\n",
    "                                     'Web Link',\n",
    "                                     'Author Profile',\n",
    "                                     'Overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c87921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for cleaning the query later\n",
    "def clean_words(words):\n",
    "    #Remove any Unicode\n",
    "    temp_words = re.sub(r'[^\\x00-\\x7F]+', ' ', words)\n",
    "    #Lowercase all words\n",
    "    temp_words = temp_words.lower()\n",
    "    #Remove all punctuation\n",
    "    temp_words = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', temp_words)\n",
    "    #Remove any digits\n",
    "    temp_words = re.sub(r'[0-9]', '', temp_words)\n",
    "    \n",
    "    return temp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60c0e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the documents\n",
    "documents_clean = []\n",
    "for d in documents:\n",
    "    #Clean the documents using the same steps as the function above\n",
    "    temp_words = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "    #Lowercase all words\n",
    "    temp_words = temp_words.lower()\n",
    "    #Remove all punctuation\n",
    "    temp_words = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', temp_words)\n",
    "    #Remove any digits\n",
    "    temp_words = re.sub(r'[0-9]', '', temp_words)\n",
    "    #append the cleaned document to the documents_clean list\n",
    "    documents_clean.append(temp_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ace241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    tokens_stop_removed = []\n",
    "    for token in tokens.split():\n",
    "        token = ps.stem(token)\n",
    "        if not token.lower() in S:\n",
    "            tokens_stop_removed.append(token)\n",
    "    return tokens_stop_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22e0fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for d in documents_clean:\n",
    "    document = remove_stopwords(d)\n",
    "    tokenized.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfa45649",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for token in tokenized:\n",
    "    final.append(TreebankWordDetokenizer().detokenize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fe7e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>york</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeitraum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 638 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9    ...  628  629  \\\n",
       "york      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "young     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "zeitraum  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "zero      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "zone      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "          630  631  632  633  634  635  636  637  \n",
       "york      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "young     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "zeitraum  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "zero      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "zone      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 638 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Term-Document Matrix with TF-IDF weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(final)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(X.T.toarray(), index=vectorizer.get_feature_names())\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c6849f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df):\n",
    "    \n",
    "    #Use clean_words function to clean query\n",
    "    q = clean_words(q)\n",
    "    \n",
    "    #Remove stopwords from query\n",
    "    q = remove_stopwords(q)\n",
    "    \n",
    "    #De-tokenize cleaned query so it is a full string for comparison\n",
    "    q = TreebankWordDetokenizer().detokenize(q)\n",
    "    \n",
    "    #Make q a list\n",
    "    q = [q]\n",
    "    \n",
    "    #Vectorise the query\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    \n",
    "    #Create an empty dictionary for the similarities\n",
    "    #Compare the query to the data TF-IDF DataFrame\n",
    "    sim = {}\n",
    "    for i in range(10):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "\n",
    "        sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for k, v in sim_sorted:\n",
    "        if v != 0.0:\n",
    "            print(\"Similarity Score:\", v)\n",
    "            print(Library.iloc[k])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33af65cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToUgh FinanciaL ClImates and WeAthEr\n",
      "Similarity Score: 0.025527672221976383\n",
      "Title               CEO Financial Experience and Firms’ Earnings M...\n",
      "Authors             Thai Nguyen, Thang Nguyen, Panagiotis Andrikop...\n",
      "Publication Date                                           7 Mar 2022\n",
      "Web Link            https://pureportal.coventry.ac.uk/en/publicati...\n",
      "Author Profile      https://pureportal.coventry.ac.uk/en/persons/t...\n",
      "Overview            This study investigates the impact of CEOs’ fi...\n",
      "Name: 4, dtype: object\n",
      "\n",
      "Similarity Score: 0.00846204124568563\n",
      "Title               Bank stock valuation theories: do they explain...\n",
      "Authors             Ken Yien Leong, Mohamed Ariff, Alireza Zarei, ...\n",
      "Publication Date                                           1 Mar 2022\n",
      "Web Link            https://pureportal.coventry.ac.uk/en/publicati...\n",
      "Author Profile      https://pureportal.coventry.ac.uk/en/persons/a...\n",
      "Overview            PurposeThe objective of this paper is to inves...\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Similarity Score: 0.007966908995753204\n",
      "Title               Competing Institutional Logics and Power Dynam...\n",
      "Authors                                 Ahmad Abras, Kelum Jayasinghe\n",
      "Publication Date                                          10 May 2022\n",
      "Web Link            https://pureportal.coventry.ac.uk/en/publicati...\n",
      "Author Profile      https://pureportal.coventry.ac.uk/en/persons/a...\n",
      "Overview            Purpose: This paper examines the historical ev...\n",
      "Name: 8, dtype: object\n",
      "\n",
      "Similarity Score: 0.005976852216597784\n",
      "Title               Corporate Governance and IFSB Standard-4: Evid...\n",
      "Authors                 Md. Harun Ur Rashid, Ruma Khanam, Hafij Ullah\n",
      "Publication Date                                          31 Jul 2021\n",
      "Web Link            https://pureportal.coventry.ac.uk/en/publicati...\n",
      "Author Profile      https://pureportal.coventry.ac.uk/en/persons/h...\n",
      "Overview            Purpose: This paper aims to examine the compli...\n",
      "Name: 9, dtype: object\n",
      "\n",
      "END OF RESULTS\n"
     ]
    }
   ],
   "source": [
    "#Run function with input from the user\n",
    "get_similar_articles(input(), df)\n",
    "\n",
    "print('END OF RESULTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712337b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
